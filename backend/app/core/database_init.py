"""
æ•°æ®åº“åˆå§‹åŒ–ä¸ç»“æ„åŒæ­¥æ¨¡å—
- æ£€æŸ¥æ¯ä¸ªè¡¨æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
- æ£€æŸ¥æ¯ä¸ªè¡¨æ˜¯å¦ä¸ºç©ºï¼Œä¸ºç©ºåˆ™åˆå§‹åŒ–æ•°æ®
- æ£€æŸ¥æ¯ä¸ªè¡¨ç»“æ„æ˜¯å¦ä¸æ¨¡å‹ä¸€è‡´ï¼Œä¸ä¸€è‡´åˆ™åŒæ­¥
"""
from loguru import logger
from sqlalchemy import text, inspect
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.engine import Inspector
from typing import Dict, Optional, Any

from app.core.database import Base, engine
from app.models import User, CaseFile, ImportTask, DocGenerateTask, OcrTask, DocTemplate
from app.core.audit import AuditLog  # ç¡®ä¿å®¡è®¡æ—¥å¿—è¡¨å‚ä¸ create_all


class DatabaseInitializer:
    """æ•°æ®åº“åˆå§‹åŒ–å™¨"""
    
    def __init__(self):
        self.engine = None
        self.inspector: Optional[Inspector] = None
        
        # è¡¨åˆå§‹åŒ–é…ç½®ï¼šè¡¨å -> é…ç½®
        self.table_configs = {
            'users': {
                'model': User,
                'data_file': 'users.json',  # JSON æ•°æ®æ–‡ä»¶
            },
            'case_files': {
                'model': CaseFile,
                'data_file': 'case_files.json',  # JSON æ•°æ®æ–‡ä»¶
            },
            'import_tasks': {
                'model': ImportTask,
                'data_file': 'import_tasks.json',  # JSON æ•°æ®æ–‡ä»¶ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
            },
            'doc_generate_tasks': {
                'model': DocGenerateTask,
                'data_file': 'doc_generate_tasks.json',  # JSON æ•°æ®æ–‡ä»¶ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
            },
            'ocr_tasks': {
                'model': OcrTask,
                'data_file': 'ocr_tasks.json',  # JSON æ•°æ®æ–‡ä»¶ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
            },
            'doc_templates': {
                'model': DocTemplate,
                'data_file': 'doc_templates.json',  # æ¨¡æ¿åˆå§‹æ•°æ®ï¼ˆå¯é€‰ï¼‰
            },
        }
    
    async def initialize(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        try:
            # ä½¿ç”¨å…¨å±€å¼•æ“
            self.engine = engine
            
            # åˆ›å»ºæ£€æŸ¥å™¨
            async with self.engine.connect() as conn:
                await conn.run_sync(self._create_inspector)
            
            logger.info("ğŸ” å¼€å§‹æ£€æŸ¥æ•°æ®åº“åˆå§‹åŒ–çŠ¶æ€...")
            
            # 1. ç¡®ä¿æ‰€æœ‰è¡¨å­˜åœ¨
            await self._ensure_tables_exist()
            
            # 2. æ£€æŸ¥å¹¶åˆå§‹åŒ–æ¯ä¸ªè¡¨çš„æ•°æ®
            await self._init_table_data()
            
            # 3. åŒæ­¥æ¯ä¸ªè¡¨çš„ç»“æ„
            await self._sync_all_tables_structure()
            
            # 4. ç»“æ„åŒæ­¥åå†æ¬¡ç¡®ä¿ä¸»é”®è‡ªå¢ï¼ˆé¿å…åŒæ­¥æ—¶è¯¯æ”¹ id æˆ–å†å²è¡¨ç¼ºè‡ªå¢ï¼‰
            async with self.engine.begin() as conn:
                await self._ensure_primary_key_autoincrement(conn)
            
            logger.info("âœ… æ•°æ®åº“åˆå§‹åŒ–æ£€æŸ¥å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def _create_inspector(self, conn):
        """åˆ›å»ºæ£€æŸ¥å™¨ï¼ˆåŒæ­¥æ–¹æ³•ï¼‰"""
        self.inspector = inspect(conn)
    
    async def _ensure_tables_exist(self):
        """ç¡®ä¿æ‰€æœ‰è¡¨å­˜åœ¨ï¼Œä¸”ä¸»é”® id ä¸ºè‡ªå¢ï¼ˆè¡¨ç»“æ„åˆå§‹åŒ–ï¼‰"""
        logger.info("ğŸ“‹ è¡¨ç»“æ„åˆå§‹åŒ–ï¼šæ£€æŸ¥å¹¶åˆ›å»ºè¡¨...")
        
        # 1. åˆ›å»ºæ‰€æœ‰è¡¨ï¼ˆè‹¥ä¸å­˜åœ¨ï¼‰
        async with self.engine.begin() as conn:
            await conn.execute(text("SET NAMES utf8mb4 COLLATE utf8mb4_unicode_ci"))
            await conn.execute(text("SET CHARACTER SET utf8mb4"))
            await conn.run_sync(Base.metadata.create_all)
        logger.info("ğŸ“‹ è¡¨ç»“æ„åˆå§‹åŒ–ï¼šcreate_all å·²å®Œæˆ")
        
        # 2. å•ç‹¬äº‹åŠ¡ï¼šç¡®ä¿ä¸»è¡¨ id åˆ—ä¸ºè‡ªå¢ï¼ˆMySQL 1364 å¿…é¡»ï¼Œä¸ create_all åˆ†å¼€æäº¤ï¼‰
        async with self.engine.begin() as conn:
            await self._ensure_primary_key_autoincrement(conn)
        
        # 3. å…¶ä»–è¡¨ç»“æ„ä¿®è¡¥
        async with self.engine.begin() as conn:
            await conn.execute(text("SET NAMES utf8mb4 COLLATE utf8mb4_unicode_ci"))
            await self._fix_doc_templates(conn)
        
        # 4. æ ¡éªŒï¼šç¡®è®¤ import_tasks.id ä¸ºè‡ªå¢
        await self._verify_autoincrement()
        logger.info("âœ… è¡¨ç»“æ„åˆå§‹åŒ–å®Œæˆ")
    
    async def _fix_doc_templates(self, conn):
        """ç¡®ä¿ doc_templates è¡¨ç»“æ„æ­£ç¡®ï¼ˆid è‡ªå¢å·²åœ¨ _ensure_primary_key_autoincrement ä¸­ç»Ÿä¸€å¤„ç†ï¼‰"""
        try:
            await conn.execute(text("""
                ALTER TABLE doc_templates 
                ADD COLUMN file_path VARCHAR(500) NULL 
                COMMENT 'æ¨¡æ¿æ–‡ä»¶è·¯å¾„ï¼ˆ.docx å…¬æ–‡æ–‡ä»¶ï¼‰'
            """))
            logger.info("âœ… doc_templates.file_path å·²æ·»åŠ ")
        except Exception as e:
            if "Duplicate column" in str(e) or "1060" in str(e):
                logger.debug("doc_templates.file_path å·²å­˜åœ¨ï¼Œè·³è¿‡")
            else:
                logger.debug(f"doc_templates.file_path ä¿®å¤è·³è¿‡: {e}")

    async def _ensure_primary_key_autoincrement(self, conn):
        """
        è¡¨ç»“æ„åˆå§‹åŒ–ï¼šç¡®ä¿ä¸»è¡¨ id åˆ—ä¸ºè‡ªå¢ã€‚
        MySQL è‹¥ id æœªè®¾ç½® AUTO_INCREMENTï¼ŒINSERT ä¸å†™ id ä¼šæŠ¥ 1364: Field 'id' doesn't have a default valueã€‚
        """
        tables_with_auto_id = ["import_tasks", "case_files", "ocr_tasks", "doc_generate_tasks", "doc_templates"]
        critical_tables = ["import_tasks", "case_files"]
        logger.info("ğŸ“‹ è¡¨ç»“æ„åˆå§‹åŒ–ï¼šç¡®ä¿ä¸»é”® id è‡ªå¢...")
        for table_name in tables_with_auto_id:
            try:
                await conn.execute(text(
                    f"ALTER TABLE `{table_name}` MODIFY COLUMN id BIGINT NOT NULL AUTO_INCREMENT"
                ))
                logger.info(f"âœ… {table_name}.id å·²è®¾ä¸ºè‡ªå¢")
            except Exception as e:
                err = str(e)
                if "1146" in err or "doesn't exist" in err.lower():
                    logger.debug(f"è¡¨ {table_name} ä¸å­˜åœ¨ï¼Œè·³è¿‡: {e}")
                else:
                    logger.warning(f"è¡¨ {table_name}.id è‡ªå¢è®¾ç½®å¤±è´¥: {e}")
                    if table_name in critical_tables:
                        raise

    async def _verify_autoincrement(self):
        """æ ¡éªŒ import_tasks / case_files çš„ id æ˜¯å¦ä¸ºè‡ªå¢ï¼Œè‹¥ä¸æ˜¯åˆ™æ‰“é”™å¹¶æŠ›é”™"""
        for table_name in ("import_tasks", "case_files"):
            async with self.engine.connect() as conn:
                r = await conn.execute(text("""
                    SELECT COLUMN_NAME, EXTRA
                    FROM INFORMATION_SCHEMA.COLUMNS
                    WHERE TABLE_SCHEMA = DATABASE() AND TABLE_NAME = :tname AND COLUMN_NAME = 'id'
                """), {"tname": table_name})
                row = r.fetchone()
            if not row:
                logger.warning(f"è¡¨ {table_name} æˆ–åˆ— id ä¸å­˜åœ¨ï¼Œè·³è¿‡è‡ªå¢æ ¡éªŒ")
                continue
            extra = (row[1] or "").lower()
            if "auto_increment" not in extra:
                msg = f"è¡¨ {table_name} çš„ id åˆ—æœªè®¾ç½®ä¸ºè‡ªå¢ã€‚è¯·åˆ é™¤è¯¥è¡¨åé‡å¯åº”ç”¨ï¼Œæˆ–æ‰‹åŠ¨æ‰§è¡Œ: ALTER TABLE {table_name} MODIFY COLUMN id BIGINT NOT NULL AUTO_INCREMENT"
                logger.error(msg)
                raise RuntimeError(msg)
            logger.info(f"âœ… æ ¡éªŒé€šè¿‡ï¼š{table_name}.id ä¸ºè‡ªå¢")
    
    async def _init_table_data(self):
        """åˆå§‹åŒ–è¡¨æ•°æ®ï¼ˆåˆ†è¡¨æ£€æŸ¥ï¼‰"""
        logger.info("ğŸ“Š æ£€æŸ¥å¹¶åˆå§‹åŒ–è¡¨æ•°æ®...")
        
        # ç¡®ä¿æ•°æ®åº“ä½¿ç”¨ UTF-8 ç¼–ç 
        async with self.engine.begin() as conn:
            # è®¾ç½®ä¼šè¯å­—ç¬¦é›†ä¸º utf8mb4ï¼ˆå¿…é¡»åœ¨æ¯ä¸ªæ“ä½œå‰è®¾ç½®ï¼‰
            await conn.execute(text("SET NAMES utf8mb4 COLLATE utf8mb4_unicode_ci"))
            await conn.execute(text("SET CHARACTER SET utf8mb4"))
            await conn.execute(text("SET character_set_client = utf8mb4"))
            await conn.execute(text("SET character_set_connection = utf8mb4"))
            await conn.execute(text("SET character_set_results = utf8mb4"))
            
            for table_name, config in self.table_configs.items():
                await self._check_and_init_table_data(conn, table_name, config)
    
    async def _check_and_init_table_data(self, conn: AsyncSession, table_name: str, config: dict):
        """æ£€æŸ¥å¹¶åˆå§‹åŒ–å•ä¸ªè¡¨çš„æ•°æ®"""
        try:
            # æ£€æŸ¥è¡¨æ˜¯å¦ä¸ºç©º
            is_empty = await self._is_table_empty(conn, table_name)
            
            if not is_empty:
                logger.debug(f"è¡¨ {table_name} å·²æœ‰æ•°æ®ï¼Œè·³è¿‡åˆå§‹åŒ–")
                return
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®æ–‡ä»¶é…ç½®
            data_file = config.get('data_file')
            if not data_file:
                logger.debug(f"è¡¨ {table_name} æ— æ•°æ®æ–‡ä»¶é…ç½®ï¼Œè·³è¿‡åˆå§‹åŒ–")
                return
            
            logger.info(f"è¡¨ {table_name} ä¸ºç©ºï¼Œå¼€å§‹åˆå§‹åŒ–...")
            
            # ä» JSON æ–‡ä»¶åŠ è½½å¹¶æ’å…¥æ•°æ®
            await self._init_table_from_json(conn, table_name, config['model'], data_file)
            
            logger.info(f"âœ… è¡¨ {table_name} æ•°æ®åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–è¡¨ {table_name} æ•°æ®å¤±è´¥: {str(e)}")
            raise
    
    async def _is_table_empty(self, conn: AsyncSession, table_name: str) -> bool:
        """æ£€æŸ¥è¡¨æ˜¯å¦ä¸ºç©º"""
        try:
            result = await conn.execute(
                text(f"SELECT COUNT(*) as count FROM `{table_name}`")
            )
            count = result.scalar()
            return count == 0
        except Exception as e:
            # å¦‚æœè¡¨ä¸å­˜åœ¨ï¼Œè¿”å› Trueï¼ˆéœ€è¦åˆå§‹åŒ–ï¼‰
            logger.debug(f"æ£€æŸ¥è¡¨ {table_name} æ˜¯å¦ä¸ºç©ºæ—¶å‡ºé”™ï¼ˆå¯èƒ½è¡¨ä¸å­˜åœ¨ï¼‰: {str(e)}")
            return True
    
    async def _init_table_from_json(self, conn: AsyncSession, table_name: str, model, json_file: str):
        """ä» JSON æ–‡ä»¶åˆå§‹åŒ–è¡¨æ•°æ®"""
        import json
        from pathlib import Path
        from datetime import datetime
        
        json_path = Path(__file__).parent.parent.parent / 'scripts' / 'data' / json_file
        
        if not json_path.exists():
            logger.warning(f"JSON æ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
            return
        
        # å†æ¬¡ç¡®ä¿è¿æ¥ä½¿ç”¨ UTF-8 ç¼–ç 
        await conn.execute(text("SET NAMES utf8mb4 COLLATE utf8mb4_unicode_ci"))
        await conn.execute(text("SET CHARACTER SET utf8mb4"))
        
        # è¯»å– JSON æ–‡ä»¶ï¼ˆç¡®ä¿ UTF-8 ç¼–ç ï¼‰
        try:
            json_content = json_path.read_text(encoding='utf-8')
            # ç¡®ä¿ JSON å†…å®¹æ˜¯ UTF-8 ç¼–ç çš„å­—ç¬¦ä¸²
            if isinstance(json_content, bytes):
                json_content = json_content.decode('utf-8')
            data_list = json.loads(json_content, strict=False)
        except UnicodeDecodeError as e:
            logger.error(f"JSON æ–‡ä»¶ç¼–ç é”™è¯¯ {json_path}: {str(e)}")
            return
        except Exception as e:
            logger.error(f"è¯»å– JSON æ–‡ä»¶å¤±è´¥ {json_path}: {str(e)}")
            return
        
        if not data_list:
            logger.debug(f"JSON æ–‡ä»¶ {json_file} ä¸ºç©ºï¼Œè·³è¿‡åˆå§‹åŒ–")
            return
        
        # è½¬æ¢æ•°æ®æ ¼å¼å¹¶æ’å…¥ï¼ˆç¡®ä¿ UTF-8 ç¼–ç ï¼‰
        inserted_count = 0
        for data_item in data_list:
            try:
                # è½¬æ¢å­—æ®µåï¼šé©¼å³°è½¬ä¸‹åˆ’çº¿
                db_data = self._convert_to_db_format(data_item, model)
                
                # ç¡®ä¿æ‰€æœ‰å­—ç¬¦ä¸²å­—æ®µéƒ½æ˜¯ UTF-8 ç¼–ç 
                db_data = self._ensure_utf8_encoding(db_data)
                
                # æ„å»º INSERT è¯­å¥
                table = model.__table__
                stmt = table.insert().values(**db_data)
                
                await conn.execute(stmt)
                inserted_count += 1
                
            except Exception as e:
                error_msg = str(e)
                import traceback
                # å¿½ç•¥é‡å¤æ’å…¥ç­‰é”™è¯¯
                if any(keyword in error_msg for keyword in [
                    'Duplicate entry', 'Duplicate key', 'already exists'
                ]):
                    logger.debug(f"æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡: {error_msg[:100]}")
                else:
                    logger.error(f"æ’å…¥æ•°æ®æ—¶å‡ºé”™: {error_msg}")
                    logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                    # ä¸ç»§ç»­æ’å…¥ï¼Œè®°å½•é”™è¯¯åè¿”å›
                    raise
        
        if inserted_count > 0:
            logger.info(f"âœ… å·²ä» {json_file} åˆå§‹åŒ–è¡¨ {table_name} ({inserted_count} æ¡è®°å½•)")
        else:
            logger.debug(f"è¡¨ {table_name} çš„æ•°æ®å·²å­˜åœ¨æˆ–æ— éœ€åˆå§‹åŒ–")
    
    def _convert_to_db_format(self, data: dict, model) -> dict:
        """å°† JSON æ•°æ®è½¬æ¢ä¸ºæ•°æ®åº“æ ¼å¼"""
        from datetime import datetime
        
        db_data = {}
        table = model.__table__
        
        for key, value in data.items():
            # JSON æ–‡ä»¶ä½¿ç”¨ä¸‹åˆ’çº¿å‘½åï¼Œç›´æ¥ä½¿ç”¨
            if key not in table.columns:
                # å¦‚æœå­—æ®µä¸å­˜åœ¨ï¼Œå°è¯•è½¬æ¢é©¼å³°å‘½å
                snake_key = self._camel_to_snake(key)
                if snake_key in table.columns:
                    key = snake_key
                else:
                    # å­—æ®µä¸å­˜åœ¨ï¼Œè·³è¿‡
                    continue
            
            # å¤„ç†å€¼
            column = table.columns[key]
            
            # å¤„ç†æ—¥æœŸæ—¶é—´å­—æ®µ
            if 'DateTime' in str(column.type) and isinstance(value, str):
                try:
                    db_data[key] = datetime.fromisoformat(value.replace(' ', 'T'))
                except:
                    db_data[key] = value
            # å¤„ç† JSON å­—æ®µ
            elif 'JSON' in str(column.type):
                db_data[key] = value  # JSON å­—æ®µç›´æ¥ä½¿ç”¨
            # å¤„ç†å…¶ä»–å­—æ®µ
            else:
                db_data[key] = value
        
        return db_data
    
    def _camel_to_snake(self, name: str) -> str:
        """å°†é©¼å³°å‘½åè½¬æ¢ä¸ºä¸‹åˆ’çº¿å‘½å"""
        import re
        # åœ¨å¤§å†™å­—æ¯å‰æ’å…¥ä¸‹åˆ’çº¿ï¼Œç„¶åè½¬å°å†™
        s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', name)
        return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()
    
    def _ensure_utf8_encoding(self, data: dict) -> dict:
        """ç¡®ä¿æ‰€æœ‰å­—ç¬¦ä¸²å­—æ®µéƒ½æ˜¯ UTF-8 ç¼–ç """
        result = {}
        for key, value in data.items():
            if isinstance(value, str):
                # å­—ç¬¦ä¸²å·²ç»æ˜¯ Unicode å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                # Python 3 ä¸­å­—ç¬¦ä¸²é»˜è®¤å°±æ˜¯ Unicodeï¼Œä¸éœ€è¦é¢å¤–è½¬æ¢
                result[key] = value
            elif isinstance(value, bytes):
                # å¦‚æœæ˜¯å­—èŠ‚ä¸²ï¼Œè§£ç ä¸º UTF-8
                try:
                    result[key] = value.decode('utf-8')
                except UnicodeDecodeError:
                    # å¦‚æœè§£ç å¤±è´¥ï¼Œå°è¯•å¿½ç•¥é”™è¯¯
                    result[key] = value.decode('utf-8', errors='ignore')
            elif isinstance(value, (dict, list)):
                # é€’å½’å¤„ç†åµŒå¥—çš„å­—å…¸å’Œåˆ—è¡¨
                result[key] = self._ensure_utf8_in_nested(value)
            else:
                result[key] = value
        return result
    
    def _ensure_utf8_in_nested(self, obj):
        """é€’å½’ç¡®ä¿åµŒå¥—ç»“æ„ä¸­çš„å­—ç¬¦ä¸²éƒ½æ˜¯ UTF-8"""
        if isinstance(obj, dict):
            return {k: self._ensure_utf8_in_nested(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._ensure_utf8_in_nested(item) for item in obj]
        elif isinstance(obj, str):
            # Python 3 å­—ç¬¦ä¸²å·²ç»æ˜¯ Unicodeï¼Œç›´æ¥è¿”å›
            return obj
        elif isinstance(obj, bytes):
            # å¦‚æœæ˜¯å­—èŠ‚ä¸²ï¼Œè§£ç ä¸º UTF-8
            try:
                return obj.decode('utf-8')
            except UnicodeDecodeError:
                return obj.decode('utf-8', errors='ignore')
        else:
            return obj
    
    
    async def _sync_all_tables_structure(self):
        """åŒæ­¥æ‰€æœ‰è¡¨çš„ç»“æ„"""
        logger.info("ğŸ”„ æ£€æŸ¥å¹¶åŒæ­¥è¡¨ç»“æ„...")
        
        async with self.engine.begin() as conn:
            for table_name, config in self.table_configs.items():
                await self._sync_table_structure(conn, table_name, config['model'])
    
    async def _sync_table_structure(self, conn: AsyncSession, table_name: str, model):
        """åŒæ­¥å•ä¸ªè¡¨çš„ç»“æ„"""
        try:
            # è·å–æœŸæœ›çš„åˆ—å®šä¹‰ï¼ˆä»æ¨¡å‹ï¼‰
            expected_columns = self._get_expected_columns(model)
            
            # è·å–å®é™…çš„åˆ—å®šä¹‰ï¼ˆä»æ•°æ®åº“ï¼‰
            actual_columns = await self._get_actual_columns(conn, table_name)
            
            # å¯¹æ¯”å·®å¼‚
            diff = self._compare_columns(expected_columns, actual_columns)
            
            if diff:
                # ç”Ÿæˆå¹¶æ‰§è¡Œ ALTER TABLE
                alter_sql = self._generate_alter_table(table_name, diff)
                if alter_sql:
                    await conn.execute(text(alter_sql))
                    logger.info(f"âœ… è¡¨ {table_name} ç»“æ„å·²åŒæ­¥")
                else:
                    logger.debug(f"è¡¨ {table_name} ç»“æ„å·®å¼‚æ— æ³•è‡ªåŠ¨åŒæ­¥ï¼Œéœ€è¦æ‰‹åŠ¨å¤„ç†")
            else:
                logger.debug(f"è¡¨ {table_name} ç»“æ„å·²æ˜¯æœ€æ–°")
                
        except Exception as e:
            logger.warning(f"åŒæ­¥è¡¨ {table_name} ç»“æ„æ—¶å‡ºé”™: {str(e)}")
            # ä¸é˜»æ­¢å¯åŠ¨ï¼Œåªè®°å½•è­¦å‘Š
    
    def _get_expected_columns(self, model) -> Dict[str, dict]:
        """è·å–æœŸæœ›çš„åˆ—å®šä¹‰ï¼ˆä» SQLAlchemy æ¨¡å‹ï¼‰"""
        columns = {}
        for column in model.__table__.columns:
            columns[column.name] = {
                'type': str(column.type),
                'nullable': column.nullable,
                'default': column.default,
                'primary_key': column.primary_key,
            }
        return columns
    
    async def _get_actual_columns(self, conn: AsyncSession, table_name: str) -> Dict[str, dict]:
        """è·å–å®é™…çš„åˆ—å®šä¹‰ï¼ˆä»æ•°æ®åº“ï¼‰"""
        columns = {}
        
        # ä½¿ç”¨ INFORMATION_SCHEMA æŸ¥è¯¢
        result = await conn.execute(text(f"""
            SELECT 
                COLUMN_NAME,
                DATA_TYPE,
                IS_NULLABLE,
                COLUMN_DEFAULT,
                COLUMN_KEY
            FROM INFORMATION_SCHEMA.COLUMNS
            WHERE TABLE_SCHEMA = DATABASE()
            AND TABLE_NAME = '{table_name}'
            ORDER BY ORDINAL_POSITION
        """))
        
        for row in result:
            columns[row.COLUMN_NAME] = {
                'type': row.DATA_TYPE,
                'nullable': row.IS_NULLABLE == 'YES',
                'default': row.COLUMN_DEFAULT,
                'primary_key': row.COLUMN_KEY == 'PRI',
            }
        
        return columns
    
    def _compare_columns(self, expected: Dict[str, dict], actual: Dict[str, dict]) -> Dict[str, Any]:  # type: ignore
        """å¯¹æ¯”åˆ—å®šä¹‰å·®å¼‚"""
        diff = {
            'add': [],  # éœ€è¦æ·»åŠ çš„åˆ—
            'modify': [],  # éœ€è¦ä¿®æ”¹çš„åˆ—
        }
        
        # æ£€æŸ¥éœ€è¦æ·»åŠ çš„åˆ—
        for col_name, col_def in expected.items():
            if col_name not in actual:
                diff['add'].append((col_name, col_def))
        
        # æ£€æŸ¥éœ€è¦ä¿®æ”¹çš„åˆ—ï¼ˆç±»å‹ã€å¯ç©ºæ€§ç­‰ï¼‰
        for col_name, expected_def in expected.items():
            if col_name in actual:
                # ä¸ä¿®æ”¹ä¸»é”®åˆ—ï¼Œé¿å… MySQL ä¸¢å¤± AUTO_INCREMENTï¼ˆMODIFY id ä¼šå»æ‰è‡ªå¢ï¼‰
                if expected_def.get('primary_key'):
                    continue
                actual_def = actual[col_name]
                # ç®€åŒ–æ¯”è¾ƒï¼šåªæ£€æŸ¥ç±»å‹å’Œå¯ç©ºæ€§
                if (expected_def['type'] != actual_def['type'] or 
                    expected_def['nullable'] != actual_def['nullable']):
                    diff['modify'].append((col_name, expected_def))
        
        return diff
    
    def _generate_alter_table(self, table_name: str, diff: Dict[str, Any]) -> Optional[str]:
        """ç”Ÿæˆ ALTER TABLE è¯­å¥"""
        alter_statements = []
        
        # æ·»åŠ æ–°åˆ—ï¼ˆè·³è¿‡ä¸»é”®ï¼Œä¸»é”®ç”±å»ºè¡¨æ—¶åˆ›å»ºï¼‰
        for col_name, col_def in diff['add']:
            if col_def.get('primary_key'):
                continue
            nullable = 'NULL' if col_def['nullable'] else 'NOT NULL'
            alter_statements.append(f"ADD COLUMN `{col_name}` {col_def['type']} {nullable}")
        
        # ä¿®æ”¹åˆ—ï¼ˆè·³è¿‡ä¸»é”®ï¼Œé¿å… MODIFY id æ—¶ MySQL ä¸¢å¤± AUTO_INCREMENTï¼‰
        for col_name, col_def in diff['modify']:
            if col_def.get('primary_key'):
                continue
            nullable = 'NULL' if col_def['nullable'] else 'NOT NULL'
            alter_statements.append(f"MODIFY COLUMN `{col_name}` {col_def['type']} {nullable}")
        
        if alter_statements:
            return f"ALTER TABLE `{table_name}` {', '.join(alter_statements)}"
        
        return None


# å…¨å±€åˆå§‹åŒ–å™¨å®ä¾‹
_initializer: Optional[DatabaseInitializer] = None


async def initialize_database():
    """åˆå§‹åŒ–æ•°æ®åº“ï¼ˆå…¥å£å‡½æ•°ï¼‰"""
    global _initializer
    
    if _initializer is None:
        _initializer = DatabaseInitializer()
    
    await _initializer.initialize()
